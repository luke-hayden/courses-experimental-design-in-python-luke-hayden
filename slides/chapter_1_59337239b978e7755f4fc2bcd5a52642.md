---
title: Insert title here
key: 59337239b978e7755f4fc2bcd5a52642

---
## Our first hypothesis test: Student's t-test

```yaml
type: "TitleSlide"
key: "5032bdf388"
```

`@lower_third`

name: Luke Hayden
title: Postdoctoral Researcher, ENS de Lyon


`@script`
Now we're going to learn our first hypothesis test: Student's t-test


---
## From observed pattern to reliable result

```yaml
type: "FullSlide"
key: "9622562193"
```

`@part1`
Where variation is present, we can see many different patterns in our data. 


Some of these are real, while others are just the results of chance. 


How can we work out whether a perceived difference is real?


`@script`
As we have seen from our exploratory data analysis, it is possible to find all kinds of patterns in our data. Some of these are expected, while others are more surprising. However, most datasets also include random variation. Knowing this, how can go from a simple observation to a reliable result?


---
## Are these groups different?

```yaml
type: "TwoColumns"
key: "80ecf60b08"
```

`@part1`
We have the weights for two groups of adults. Both samples contain variation. 

**Sample A:**
```
[58.2, 69.8,64.7,69.6,78.1]
```
**Sample B:**
```
[83.7,78.5, 78.6, 86.9, 84.4]
```

Are these different?


`@part2`
![](https://raw.githubusercontent.com/luke-hayden/courses-experimental-design-in-python-luke-hayden/master/bar.png)


`@script`
Let's take a really simple example. Let's say we have the body weights of two samples from two groups of people, A and B. From our small number of samples, we seem to see a trend, whereby the group mean for sample B is bigger than that for sample A. But is this difference a real one, or just some random variation?


---
## Null hypothesis vs alternative hypothesis

```yaml
type: "TwoRowsTwoColumns"
key: "1adfecb6ee"
```

`@part1`
**Null hypothesis**

No difference between group A and group B. 
Both groups have the same mean population-level mean. 

_A = B_


`@part2`
**![](https://raw.githubusercontent.com/luke-hayden/courses-experimental-design-in-python-luke-hayden/master/unimod.png)**


`@part3`
**Alternative hypothesis**

This is the "more interesting" alternative: these two populations are meaningfully different. 

_A != B_


`@part4`
![](https://raw.githubusercontent.com/luke-hayden/courses-experimental-design-in-python-luke-hayden/master/bimod.png)


`@script`
In order to draw a conclusion, we will need to distinguish between two cases, two different hypotheses. In statistics, our starting point is always the "null hypothesis" that there isn't anything interesting going on and that the patterns that we have seen are just the product of random chance. With enough evidence, we can reject this null hypothesis and turn to the more interesting alternative hypothesis; that the difference between these two samples represents a real difference between the populations.


---
## Some statistical terms

```yaml
type: "FullSlide"
key: "17c0b4b679"
```

`@part1`
**p-value**

A p-value can quantify how likely it is that we would observe a particular pattern under the **null hypothesis.** A low p-value means that it's very unlikely that the observed pattern would have emerged under the null hypothesis. 

**alpha**

Alpha is the term used for the crucial threshold of p-value, below which we would reject the null hypothesis. Usually: 

_alpha < 0.05: reject null hypothesis_


`@script`
But at what point do we know to reject the null hypothesis? Here we turn to two statistics. The first, the p-value, represents the likelihood that we would encounter the distribution of values seen in our samples if the null hypothesis was correct. While we can't be 100% sure that our pattern couldn't have emerged due to random chance, we can quantify the probability that random chance would produce our given pattern. This is our p-value. Where the p-value is very small, we can fairly sure that random chance didn't cause the pattern we see. 
We interpret the p-value with the help of alpha. This is the threshold below which we reject the null hypothesis. A standard value for alpha is 0.05. That is to say that, below a 5% probability that random chance would produce the pattern observed, it is usually considered safe to reject the null hypothesis.


---
## Student's t-test (two-sample)

```yaml
type: "TwoColumns"
key: "ee2f383474"
```

`@part1`
**Background**

Invented by William Sealy Gosset, while working on methods to test beer quality. 

**Use**

Given two samples, tests whether the means of the populations from which these samples are drawn are equal.


`@part2`
**Implementing a two-sample t-test**

```
from scipy import stats 

stats.ttest_ind(X, Y)

[t_stat, p-value]
```


`@script`
For this type of data, where we are comparing two sets of values for a continuous variable, we will use Student's t-test. This test was invented by William Sealy Gosset as a means of monitoring beer quality for the Guinness brewery. A noble endeavour, to be sure! With this method, given two samples of values, we can test the likelihood that there is a meaningful difference between the populations from which they are drawn. 
In Python, we can use the stats module from the Scipy package to perform t-tests. The function we'll use is ttest_ind and we'll give it two arrays in order to carry out a two-sample t-test. This will yield an array of length two, containing the p-value at index 1.


---
## Implementing a t-test

```yaml
type: "FullCodeSlide"
key: "d94ce2df22"
```

`@part1`
```
from scipy import stats 


Sample_A= [58.2, 69.8,64.7,69.6,78.1]
Sample_B=[83.7,78.5, 78.6, 86.9, 84.4]


t_result=stats.ttest_ind(Sample_A, Sample_B)

alpha= 0.05

if (t_result[1] < alpha):
    print("A and B are different!")

```

>A and B are different!


`@script`
Now we're ready to answer the question. Are the members of group B heavier than the members of group A? First we import stats. Then we define our two arrays, naming the Samples A and B. Then we perform our test, using Sample A and B as the two arguments for the ttest function. We save the result of this as t_result. Finally, using a standard alpha value of 0.05, we test whether our p-value falls under alpha. It does! It seems that this difference may have some validity.


---
## Now let's try it out!

```yaml
type: "FinalSlide"
key: "5dc84036b6"
```

`@script`
Now it's time for you to try it out.

